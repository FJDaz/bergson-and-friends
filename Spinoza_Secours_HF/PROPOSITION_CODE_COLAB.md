# üìù Proposition : Code Colab pour Spinoza Secours

**Date :** 21 novembre 2025  
**Status :** ‚è∏Ô∏è **PROPOSITION** - En attente validation  
**‚ö†Ô∏è NE PAS MODIFIER LE CODE COLAB SANS VALIDATION**

---

## üéØ Objectif

Cr√©er un code Colab complet qui utilise le **prompt syst√®me hybride optimis√©** pour Spinoza Secours.

---

## üìã Contenu Propos√©

### 1. **Installation D√©pendances**
```python
!pip install -q pyngrok fastapi uvicorn transformers peft accelerate bitsandbytes torch
```

### 2. **Imports**
```python
from pyngrok import ngrok
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import threading, uvicorn, random, time, re, torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
```

### 3. **Configuration ngrok**
```python
NGROK_TOKEN = "TON_TOKEN_ICI"
ngrok.set_auth_token(NGROK_TOKEN)
!lsof -ti:8000 | xargs kill -9 2>/dev/null; sleep 1
```

### 4. **Prompt Syst√®me Hybride** (depuis `prompt_systeme_hybride.py`)
- `SYSTEM_PROMPT_SPINOZA` (~250 tokens)
- `INSTRUCTIONS_CONTEXTUELLES` (accord/confusion/r√©sistance/neutre)
- `INSTRUCTION_RAG` (optionnel)
- `construire_prompt_complet(contexte, use_rag_instruction=True)`

### 5. **D√©tection Contexte**
```python
def detecter_contexte(user_input: str) -> str:
    # Retourne "accord", "confusion", "resistance", "neutre"
```

### 6. **Post-Processing**
- `nettoyer_reponse(text)` - Nettoie annotations, emojis, espaces
- `limiter_phrases(text, max_phrases=3)` - Limite √† 3 phrases

### 7. **Chargement Mod√®le**
```python
@torch.no_grad()
def load_model():
    # Charge Mistral 7B + LoRA
    # Device: CPU (ou CUDA si GPU)
    return model, tokenizer
```

### 8. **Fonction `spinoza_repond(message)`**
- D√©tecte contexte
- Construit prompt adaptatif
- G√©n√®re r√©ponse avec mod√®le
- Post-processe
- Retourne r√©ponse nettoy√©e

### 9. **API FastAPI**
- `/health` - V√©rification √©tat
- `/init` - Initialisation conversation
- `/chat` - POST avec message utilisateur
- CORS configur√©

### 10. **Lancement Serveur + ngrok**
- Thread background pour FastAPI
- Tunnel ngrok sur port 8000
- Affichage URL publique

---

## ‚öôÔ∏è Param√®tres Configurables

| Param√®tre | Valeur Propos√©e | Alternative |
|-----------|----------------|-------------|
| `max_new_tokens` | 150 | 100-200 selon besoin |
| `temperature` | 0.7 | 0.5-0.9 |
| `top_p` | 0.9 | 0.8-0.95 |
| `use_rag_instruction` | `True` | `False` pour √©conomie max |
| `device` | `"cpu"` | `"cuda"` si GPU |
| `adapter_name` | `"FJDaz/mistral-7b-philosophes-lora"` | √Ä ajuster si diff√©rent |

---

## üìä Estimation Tokens

| Composant | Tokens |
|-----------|--------|
| Prompt syst√®me base | ~250 |
| Instruction contextuelle | ~30-50 |
| Instruction RAG | ~50 |
| **Total prompt** | **~330-350** |
| Historique (4 √©changes) | ~300 |
| Message utilisateur | ~50 |
| **Total par requ√™te** | **~680-700** |

---

## ‚úÖ Avantages

1. **Prompt optimis√©** : ~250 tokens (vs ~400 pour version compl√®te)
2. **Adaptatif** : S'adapte au contexte (accord/confusion/r√©sistance/neutre)
3. **Premi√®re personne** : Explicite dans le prompt
4. **Sch√®mes logiques** : Int√©gr√©s dans le prompt
5. **√âconomie tokens** : RAG par instructions (pas d'injection)

---

## ‚ö†Ô∏è Points d'Attention

1. **Token ngrok** : √Ä remplacer par le vrai token
2. **Adapter LoRA** : V√©rifier le nom exact de l'adapter
3. **Device** : CPU par d√©faut (changer en CUDA si GPU)
4. **RAG** : Actuellement par instructions (pas d'injection passages)

---

## üîÑ Modifications Possibles

### Option A : RAG Disabled (√âconomie Max)
```python
# Dans spinoza_repond()
system_prompt = construire_prompt_complet(contexte, use_rag_instruction=False)
```
**√âconomie :** ~50 tokens

### Option B : Prompt Minimal (√âconomie Max)
```python
# Utiliser version minimaliste (~80 tokens)
SYSTEM_PROMPT_MINIMAL = """Tu ES Spinoza. Premi√®re personne. Tutoie l'√©l√®ve..."""
```
**√âconomie :** ~170 tokens

### Option C : RAG S√©lectif (Si besoin)
```python
# Ajouter recherche RAG si contexte confusion/accord
if contexte in ["confusion", "accord"]:
    # Recherche RAG + injection passages
```

---

## üìù Structure Fichier Propos√©e

```
colab_spinoza_secours_complet.py
‚îú‚îÄ‚îÄ Installation d√©pendances
‚îú‚îÄ‚îÄ Imports
‚îú‚îÄ‚îÄ Config ngrok
‚îú‚îÄ‚îÄ Prompt syst√®me hybride
‚îú‚îÄ‚îÄ D√©tection contexte
‚îú‚îÄ‚îÄ Post-processing
‚îú‚îÄ‚îÄ Chargement mod√®le
‚îú‚îÄ‚îÄ Fonction spinoza_repond()
‚îú‚îÄ‚îÄ API FastAPI
‚îî‚îÄ‚îÄ Lancement serveur + ngrok
```

---

## ‚ùì Questions pour Validation

1. **Adapter LoRA** : Quel est le nom exact de l'adapter √† utiliser ?
2. **Device** : CPU ou CUDA (GPU disponible ?)
3. **RAG** : Instructions seulement ou injection passages ?
4. **Tokens** : Priorit√© √©conomie ou qualit√© ?
5. **Param√®tres g√©n√©ration** : `max_new_tokens=150` OK ?

---

**Status :** ‚è∏Ô∏è En attente validation avant impl√©mentation

