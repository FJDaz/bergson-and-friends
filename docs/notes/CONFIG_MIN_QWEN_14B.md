# ‚öôÔ∏è Configuration Minimale - Qwen 14B

**Mod√®le :** Qwen/Qwen2.5-14B-Instruct  
**Fine-tuning :** LoRA Spinoza Niveau B (adapter)

---

## üìä Besoins en VRAM par Quantization

| Quantization | VRAM Mod√®le | VRAM + Overhead | GPU Minimum | Status |
|--------------|-------------|-----------------|-------------|--------|
| **FP16** (sans quant) | ~28GB | ~30GB | A100 (40GB) | ‚ùå Trop lourd |
| **8-bit** | ~14GB | ~16GB | **A10G (24GB)** ‚úÖ | ‚ö†Ô∏è T4 limite |
| **4-bit** | ~7GB | ~9GB | **T4 (16GB)** ‚úÖ | ‚úÖ Recommand√© T4 |

---

## ‚úÖ Configuration 8-bit (Actuelle)

### VRAM Requise
- **Mod√®le :** ~14GB
- **Overhead :** ~2GB (activations, cache)
- **Total :** ~16GB minimum

### GPU Compatibles

#### ‚úÖ **A10G (24GB VRAM)** - RECOMMAND√â
- **VRAM disponible :** 24GB
- **Marge :** 8GB (confortable)
- **Status :** ‚úÖ Fonctionne parfaitement
- **Co√ªt HF :** ~$1.00/h
- **Co√ªt RunPod :** ~$1.00/h

#### ‚ö†Ô∏è **T4 Small (16GB VRAM)** - LIMITE
- **Sp√©cifications :** 4 vCPU, 15GB RAM, 16GB VRAM
- **VRAM disponible :** 16GB
- **Marge :** 0GB (juste suffisant th√©oriquement)
- **Status :** ‚ùå **Ne fonctionne PAS en 8-bit** (exp√©rience confirm√©e)
- **Probl√®me :** Mod√®le dispatch√© sur CPU/disk ‚Üí Runtime error
- **Raison :** Overhead syst√®me + activations = d√©passement
- **Note RAM :** 15GB RAM peut √™tre limite si offload CPU n√©cessaire
- **Solution :** Utiliser 4-bit (passe avec marge)

#### ‚ö†Ô∏è **T4 Medium (16GB VRAM)** - LIMITE
- **Sp√©cifications :** 8 vCPU, 30GB RAM, 16GB VRAM
- **VRAM disponible :** 16GB
- **Marge :** 0GB (juste suffisant th√©oriquement)
- **Status :** ‚ùå **Ne fonctionne PAS en 8-bit** (exp√©rience confirm√©e)
- **Probl√®me :** Mod√®le dispatch√© sur CPU/disk ‚Üí Runtime error
- **Raison :** Overhead syst√®me + activations = d√©passement
- **Note RAM :** 30GB RAM largement suffisante
- **Solution :** Utiliser 4-bit (passe avec marge)

### Code Configuration 8-bit

```python
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-14B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16,
)
```

---

## ‚úÖ Configuration 4-bit (Minimale pour T4)

### VRAM Requise
- **Mod√®le :** ~7GB
- **Overhead :** ~2GB
- **Total :** ~9GB minimum

### GPU Compatibles

#### ‚úÖ **T4 Small (16GB VRAM)** - FONCTIONNE (avec pr√©caution)
- **Sp√©cifications :** 4 vCPU, 15GB RAM, 16GB VRAM
- **VRAM disponible :** 16GB
- **Marge :** 7GB (confortable avec 4-bit)
- **Status :** ‚úÖ Devrait fonctionner en 4-bit (version test√©e dans archive)
- **Co√ªt HF :** ~$0.40/h
- **Co√ªt RunPod :** ~$0.30/h
- **Note RAM :** 15GB RAM peut √™tre limite si offload CPU n√©cessaire (√©viter)
- **Recommandation :** Forcer tout sur GPU (pas d'offload CPU)

#### ‚úÖ **T4 Medium (16GB VRAM)** - FONCTIONNE
- **Sp√©cifications :** 8 vCPU, 30GB RAM, 16GB VRAM
- **VRAM disponible :** 16GB
- **Marge :** 7GB (confortable avec 4-bit)
- **Status :** ‚úÖ Devrait fonctionner en 4-bit (version test√©e dans archive)
- **Co√ªt HF :** ~$0.40/h
- **Co√ªt RunPod :** ~$0.30/h
- **Note :** RAM (30GB) largement suffisante pour le mod√®le

#### ‚úÖ **A10G (24GB VRAM)** - SURDIMENSIONN√â
- **VRAM disponible :** 24GB
- **Marge :** 15GB (tr√®s confortable)
- **Status :** ‚úÖ Fonctionne (mais 8-bit pr√©f√©rable pour qualit√©)

### Code Configuration 4-bit

```python
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # 4-bit au lieu de 8-bit
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,  # Double quantization pour meilleure qualit√©
    bnb_4bit_quant_type="nf4",  # NormalFloat4 - meilleure qualit√©
)

model = AutoModelForCausalLM.from_pretrained(
    "Qwen/Qwen2.5-14B-Instruct",
    quantization_config=quantization_config,
    device_map="auto",
    torch_dtype=torch.float16,
)
```

**Note :** Version 4-bit trouv√©e dans `spinoza_NB_archive/version_23f53af/app.py` (non d√©ploy√©e)

---

## üéØ Recommandations par Cas d'Usage

### Cas 1 : Budget Limit√© (T4 Small/Medium)

**Configuration :** 4-bit  
**GPU :** 
- **T4 Small :** 4 vCPU, 15GB RAM, 16GB VRAM (‚ö†Ô∏è RAM limite)
- **T4 Medium :** 8 vCPU, 30GB RAM, 16GB VRAM (‚úÖ Recommand√©)  
**Co√ªt :** ~$0.30-0.40/h  
**Qualit√© :** L√©g√®rement inf√©rieure √† 8-bit (acceptable)

**Code :**
```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)
```

### Cas 2 : Qualit√© Optimale (A10G)

**Configuration :** 8-bit  
**GPU :** A10G (24GB)  
**Co√ªt :** ~$1.00/h  
**Qualit√© :** Meilleure (moins de perte de pr√©cision)

**Code :**
```python
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
)
```

### Cas 3 : Usage Ponctuel (D√©mos)

**Configuration :** 4-bit sur T4  
**Avantage :** Co√ªt r√©duit (~$0.30/h vs $1.00/h)  
**Qualit√© :** Suffisante pour d√©mos

---

## ‚ö†Ô∏è Limitations Connues

### 8-bit sur T4 : ‚ùå Ne Fonctionne PAS

**Exp√©rience confirm√©e :**
- Space `spinoza_NB` (T4) ‚Üí Runtime error
- Mod√®le dispatch√© sur CPU/disk
- Cause : VRAM insuffisante (14GB + overhead > 16GB)

**Solution :** Utiliser 4-bit ou passer √† A10G

### 4-bit : Qualit√© L√©g√®rement Inf√©rieure

**Trade-off :**
- ‚úÖ Moins de VRAM (passe sur T4)
- ‚úÖ Moins cher (~$0.30/h vs $1.00/h)
- ‚ö†Ô∏è L√©g√®re perte de qualit√© (acceptable pour la plupart des cas)

---

## üìã Comparaison Rapide

| Crit√®re | 8-bit (A10G) | 4-bit (T4 Small) | 4-bit (T4 Medium) |
|---------|-------------|------------------|-------------------|
| **VRAM** | ~16GB | ~9GB | ~9GB |
| **GPU** | A10G (24GB) | T4 Small (16GB VRAM) | T4 Medium (16GB VRAM) |
| **CPU/RAM** | - | 4 vCPU, 15GB RAM | 8 vCPU, 30GB RAM |
| **Co√ªt/h** | $1.00 | $0.30-0.40 | $0.30-0.40 |
| **Qualit√©** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| **Status** | ‚úÖ Fonctionne | ‚ö†Ô∏è Fonctionne (RAM limite) | ‚úÖ Devrait fonctionner |
| **Recommand√© pour** | Production | Budget tr√®s limit√© | Budget limit√© |

---

## üîß Migration 8-bit ‚Üí 4-bit

### Modifications √† Apporter

**Fichier :** `app.py` (ou `bergsonAndFriends/app.py`)

**Avant (8-bit) :**
```python
quantization_config = BitsAndBytesConfig(
    load_in_8bit=True,
    llm_int8_threshold=6.0,
    llm_int8_has_fp16_weight=False,
)
```

**Apr√®s (4-bit) :**
```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
)

# Pour T4 Small (RAM limite) : Forcer tout sur GPU
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    quantization_config=quantization_config,
    device_map="auto",  # Ou device_map={0: "15GiB", "cpu": "0GiB"} pour forcer GPU
    torch_dtype=torch.float16,
    max_memory={0: "15GiB", "cpu": "0GiB"},  # Forcer GPU uniquement (T4 Small)
)
```

**Aucune autre modification n√©cessaire** (le reste du code reste identique)

---

## üí° Conclusion

### Configuration Minimale Recommand√©e

**Pour T4 Small (4 vCPU, 15GB RAM, 16GB VRAM) :**
- ‚úÖ **4-bit** : ~7GB VRAM ‚Üí Passe avec marge
- ‚ö†Ô∏è **Attention RAM :** 15GB peut √™tre limite (forcer GPU uniquement)
- ‚ùå **8-bit** : ~14GB VRAM ‚Üí Ne passe PAS (confirm√©)

**Pour T4 Medium (8 vCPU, 30GB RAM, 16GB VRAM) :**
- ‚úÖ **4-bit** : ~7GB VRAM ‚Üí Passe avec marge (recommand√©)
- ‚úÖ **RAM :** 30GB largement suffisante
- ‚ùå **8-bit** : ~14GB VRAM ‚Üí Ne passe PAS (confirm√©)

**Pour A10G (24GB VRAM) :**
- ‚úÖ **8-bit** : ~14GB VRAM ‚Üí Passe avec marge (recommand√©)
- ‚úÖ **4-bit** : ~7GB VRAM ‚Üí Passe mais surdimensionn√©

### Recommandation Finale

- **Budget tr√®s limit√© :** 4-bit sur T4 Small (~$0.30/h) ‚ö†Ô∏è RAM limite
- **Budget limit√© :** 4-bit sur T4 Medium (~$0.30-0.40/h) ‚úÖ Recommand√©
- **Qualit√© optimale :** 8-bit sur A10G (~$1.00/h)
- **Usage actuel :** 8-bit sur A10G (fonctionne parfaitement)

---

**Derni√®re mise √† jour :** Novembre 2025  
**Source :** Analyse code + exp√©rience Space `spinoza_NB` (T4) vs `bergsonAndFriends` (A10G)

