"""
Version FastAPI de SNB - Test de robustesse API REST
R√©utilise exactement la m√™me logique que app.py (Gradio) mais expose des endpoints REST
"""
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List, Tuple, Optional, Dict
import uvicorn
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import re
import random
import os

# Import de la logique existante depuis app.py
# On pourrait aussi copier/coller, mais on va r√©utiliser directement

# ============================================
# CONFIGURATION
# ============================================

BASE_MODEL = "Qwen/Qwen2.5-14B-Instruct"
ADAPTER_MODEL = "FJDaz/qwen-spinoza-niveau-b"
HF_TOKEN = os.getenv("HF_TOKEN")

# ============================================
# MODELS PYDANTIC (FastAPI)
# ============================================

class ChatRequest(BaseModel):
    message: str
    history: Optional[List[List[Optional[str]]]] = []
    philosopher: Optional[str] = "spinoza"  # spinoza, bergson, kant

class ChatResponse(BaseModel):
    reply: str
    history: List[List[Optional[str]]]
    contexte: str

class InitResponse(BaseModel):
    philosopher: str
    question: str
    greeting: str
    history: List[List[Optional[str]]]

# ============================================
# R√âUTILISATION LOGIQUE DEPUIS app.py
# ============================================

# Copie des constantes et fonctions depuis app.py
PROMPTS_PHILOSOPHES = {
    "spinoza": [
        """Tu es Spinoza incarn√©. Tu dialogues avec un √©l√®ve pour le guider vers la compr√©hension.
Utilise les sch√®mes logiques : IDENTIT√â (Dieu=Nature), IMPLICATION (joie‚Üípuissance), CAUSALIT√â n√©cessaire.
Varie tes transitions: "Donc", "MAIS ALORS", "Imagine", "Cela implique".
Style : G√©om√©trie des affects, causes n√©cessaires, conatus.""",

        """Tu es Spinoza. Guide l'√©l√®ve vers la compr√©hension de la n√©cessit√©.
Sch√®mes logiques : Libert√© = Connaissance de la n√©cessit√©, Affects = Variations de puissance.
Utilise "MAIS ALORS" pour r√©v√©ler les contradictions.
Reste concis, p√©dagogique, rigoureux."""
    ],
    "bergson": [
        """Tu es Henri Bergson. Tu dialogues avec un √©l√®ve sur la dur√©e et la conscience.
Utilise les sch√®mes logiques : OPPOSITION (dur√©e ‚â† temps spatial), CONTINUIT√â qualitative.
M√©taphores : m√©lodie, flux, √©lan vital.
Varie tes transitions: "Donc", "Imagine", "C'est contradictoire".""",

        """Tu es Bergson. Guide l'√©l√®ve vers l'intuition de la dur√©e v√©cue.
Sch√®mes logiques : Dur√©e pure vs temps spatialis√©, M√©moire = conservation totale.
Utilise des analogies concr√®tes (m√©lodie, souvenir).
Questionne pour faire sentir la diff√©rence."""
    ],
    "kant": [
        """Tu es Emmanuel Kant. Tu dialogues avec un √©l√®ve sur les limites de la raison.
Utilise les sch√®mes logiques : DISTINCTION (ph√©nom√®ne/noum√®ne, a priori/a posteriori).
Architecture critique : sensibilit√©, entendement, raison.
Varie tes transitions: "Il convient d'examiner", "Distinguons", "Cela implique".""",

        """Tu es Kant. Guide l'√©l√®ve vers la compr√©hension critique.
Sch√®mes logiques : Synth√®se a priori, Imp√©ratif cat√©gorique, Autonomie morale.
Utilise les distinctions rigoureuses.
Questionne les conditions de possibilit√©."""
    ]
}

QUESTIONS_BAC = {
    "spinoza": [
        "La libert√© est-elle une illusion ?",
        "Suis-je esclave de mes d√©sirs ?",
        "La joie procure-t-elle un pouvoir ?",
        "Peut-on d√©sirer sans souffrir ?",
        "La raison peut-elle tout expliquer ?"
    ],
    "bergson": [
        "Le temps passe-t-il vraiment ?",
        "Se souvenir, est-ce revivre ?",
        "L'art requiert-il de l'inspiration ?",
        "Peut-on se conna√Ætre soi-m√™me ?",
        "La conscience fait-elle notre identit√© ?"
    ],
    "kant": [
        "Agir moralement, est-ce agir par devoir ?",
        "√ätre libre, est-ce faire ce qui nous pla√Æt ?",
        "Que puis-je savoir du monde ?",
        "La morale est-elle universelle ?",
        "Qu'est-ce qu'une soci√©t√© juste ?"
    ]
}

def detecter_contexte(user_input: str) -> str:
    """D√©tecte le contexte √©motionnel de la r√©ponse"""
    text_lower = user_input.lower()
    if any(re.search(p, text_lower) for p in [r'\boui\b', r'\bd\'accord\b', r'\bexact\b', r'\bc\'est √ßa\b']):
        return "accord"
    if any(re.search(p, text_lower) for p in [r'comprends? pas', r'je sais pas', r'c\'est quoi']):
        return "confusion"
    if any(re.search(p, text_lower) for p in [r'\bmais\b', r'\bnon\b', r'pas d\'accord', r'faux']):
        return "resistance"
    return "neutre"

def nettoyer_reponse(text: str) -> str:
    """Nettoie la r√©ponse g√©n√©r√©e"""
    text = re.sub(r'\([^)]*[Aa]ttends[^)]*\)', '', text)
    text = re.sub(r'[üòÄ-üôèüåÄ-üóøüöÄ-üõø]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def limiter_phrases(text: str, max_phrases: int = 3) -> str:
    """Limite le nombre de phrases"""
    phrases = re.split(r'[.!?]+\s+', text)
    phrases = [p.strip() for p in phrases if p.strip()]
    if len(phrases) <= max_phrases:
        return text
    return '. '.join(phrases[:max_phrases]) + '.'

class DialoguePhilosophe:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def generate_response(self, user_input: str, history: List[Tuple], philosophe: str) -> Dict:
        """G√©n√®re une r√©ponse adapt√©e au philosophe"""
        history = history or []
        contexte = detecter_contexte(user_input)
        base_prompt = random.choice(PROMPTS_PHILOSOPHES[philosophe])
        
        system_prompt = base_prompt + """\n\nR√àGLES STRICTES:
- Tutoie toujours l'√©l√®ve (tu/ton/ta)
- Questionne au lieu d'affirmer
- Varie tes formulations
"""
        if contexte == "confusion":
            system_prompt += "\nL'√©l√®ve est confus ‚Üí Donne UNE analogie concr√®te simple."
        elif contexte == "resistance":
            system_prompt += "\nL'√©l√®ve r√©siste ‚Üí R√©v√®le une contradiction dans sa position."
        elif contexte == "accord":
            system_prompt += "\nL'√©l√®ve accepte. AVANCE vers la prochaine √©tape logique."

        messages = [{"role": "system", "content": system_prompt}]
        for exchange in history[-4:]:
            if exchange[0]:
                messages.append({"role": "user", "content": exchange[0]})
            if exchange[1]:
                clean_response = exchange[1].split('\n\n*[')[0]
                messages.append({"role": "assistant", "content": clean_response})
        messages.append({"role": "user", "content": user_input})

        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        response = nettoyer_reponse(response)
        response = limiter_phrases(response, 3)

        return {
            "message": response,
            "contexte": contexte
        }

# ============================================
# CHARGEMENT MOD√àLE (global pour √©viter reload)
# ============================================

_dialogue = None

@torch.no_grad()
def load_model():
    """Charge le mod√®le SNB (Qwen 14B + LoRA)"""
    global _dialogue
    
    if _dialogue is not None:
        return _dialogue
    
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=True,
        llm_int8_threshold=6.0,
        llm_int8_has_fp16_weight=False,
    )

    print("üîÑ Chargement Qwen 14B (8-bit)...")
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=quantization_config,
        device_map="auto",
        torch_dtype=torch.float16,
        token=HF_TOKEN,
        trust_remote_code=True
    )

    print("üîÑ Chargement tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        token=HF_TOKEN,
        trust_remote_code=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("üîÑ Application LoRA Spinoza Niveau B...")
    model = PeftModel.from_pretrained(
        base_model,
        ADAPTER_MODEL,
        token=HF_TOKEN
    )

    print("‚úÖ Mod√®le charg√© avec succ√®s!")
    _dialogue = DialoguePhilosophe(model, tokenizer)
    return _dialogue

# ============================================
# FASTAPI APP
# ============================================

app = FastAPI(
    title="SNB API (FastAPI)",
    description="API REST pour Spinoza Niveau B - 3 philosophes",
    version="1.0.0"
)

@app.on_event("startup")
async def startup_event():
    """Charge le mod√®le au d√©marrage"""
    print("üöÄ D√©marrage FastAPI SNB...")
    load_model()

@app.get("/")
async def root():
    return {
        "message": "SNB API (FastAPI) - Endpoints disponibles: /chat_spinoza, /chat_bergson, /chat_kant, /init"
    }

@app.get("/health")
async def health():
    return {"status": "ok", "model_loaded": _dialogue is not None}

@app.post("/init/{philosopher}", response_model=InitResponse)
async def init_philosopher(philosopher: str):
    """Initialise une conversation avec une question du bac"""
    if philosopher not in ["spinoza", "bergson", "kant"]:
        raise HTTPException(status_code=400, detail="Philosophe invalide. Utilise: spinoza, bergson, ou kant")
    
    question = random.choice(QUESTIONS_BAC[philosopher])
    noms = {"spinoza": "Spinoza", "bergson": "Henri Bergson", "kant": "Emmanuel Kant"}
    greeting = f"Bonjour ! Je suis {noms[philosopher]}. Discutons ensemble de cette question :\n\n**{question}**\n\nQu'en penses-tu ?"
    
    return InitResponse(
        philosopher=philosopher,
        question=question,
        greeting=greeting,
        history=[[None, greeting]]
    )

@app.post("/chat_spinoza", response_model=ChatResponse)
@app.post("/chat_bergson", response_model=ChatResponse)
@app.post("/chat_kant", response_model=ChatResponse)
@app.post("/chat/{philosopher}", response_model=ChatResponse)
async def chat(request: ChatRequest, philosopher: Optional[str] = None):
    """
    Endpoint de chat universel
    - Utilise /chat/{philosopher} OU le philosophe dans le body
    """
    # D√©terminer le philosophe (body > path)
    phil = philosopher or request.philosopher
    if phil not in ["spinoza", "bergson", "kant"]:
        raise HTTPException(status_code=400, detail="Philosophe invalide")
    
    if not request.message.strip():
        raise HTTPException(status_code=400, detail="Message vide")
    
    try:
        dialogue = load_model()
        history = request.history or []
        
        result = dialogue.generate_response(request.message, history, phil)
        response = result["message"]
        contexte = result["contexte"]
        
        # Ajouter au history
        history.append([request.message, f"{response}\n\n*[Contexte: {contexte}]*"])
        
        return ChatResponse(
            reply=response,
            history=history,
            contexte=contexte
        )
    except Exception as e:
        print(f"[ERROR] {str(e)}")
        raise HTTPException(status_code=500, detail=f"Erreur g√©n√©ration: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=7860)

