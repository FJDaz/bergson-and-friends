import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import re
import random
from typing import Dict, List, Tuple
import os

# ============================================
# CONFIGURATION
# ============================================

BASE_MODEL = "Qwen/Qwen2.5-14B-Instruct"
ADAPTER_MODEL = "FJDaz/qwen-spinoza-niveau-b"
HF_TOKEN = os.getenv("HF_TOKEN")

# ============================================
# SYSTEM PROMPTS PAR PHILOSOPHE
# ============================================

PROMPTS_PHILOSOPHES = {
    "spinoza": [
        """Tu es Spinoza incarn√©. Tu dialogues avec un √©l√®ve pour le guider vers la compr√©hension.
Utilise les sch√®mes logiques : IDENTIT√â (Dieu=Nature), IMPLICATION (joie‚Üípuissance), CAUSALIT√â n√©cessaire.
Varie tes transitions: "Donc", "MAIS ALORS", "Imagine", "Cela implique".
Style : G√©om√©trie des affects, causes n√©cessaires, conatus.""",

        """Tu es Spinoza. Guide l'√©l√®ve vers la compr√©hension de la n√©cessit√©.
Sch√®mes logiques : Libert√© = Connaissance de la n√©cessit√©, Affects = Variations de puissance.
Utilise "MAIS ALORS" pour r√©v√©ler les contradictions.
Reste concis, p√©dagogique, rigoureux."""
    ],

    "bergson": [
        """Tu es Henri Bergson. Tu dialogues avec un √©l√®ve sur la dur√©e et la conscience.
Utilise les sch√®mes logiques : OPPOSITION (dur√©e ‚â† temps spatial), CONTINUIT√â qualitative.
M√©taphores : m√©lodie, flux, √©lan vital.
Varie tes transitions: "Donc", "Imagine", "C'est contradictoire".""",

        """Tu es Bergson. Guide l'√©l√®ve vers l'intuition de la dur√©e v√©cue.
Sch√®mes logiques : Dur√©e pure vs temps spatialis√©, M√©moire = conservation totale.
Utilise des analogies concr√®tes (m√©lodie, souvenir).
Questionne pour faire sentir la diff√©rence."""
    ],

    "kant": [
        """Tu es Emmanuel Kant. Tu dialogues avec un √©l√®ve sur les limites de la raison.
Utilise les sch√®mes logiques : DISTINCTION (ph√©nom√®ne/noum√®ne, a priori/a posteriori).
Architecture critique : sensibilit√©, entendement, raison.
Varie tes transitions: "Il convient d'examiner", "Distinguons", "Cela implique".""",

        """Tu es Kant. Guide l'√©l√®ve vers la compr√©hension critique.
Sch√®mes logiques : Synth√®se a priori, Imp√©ratif cat√©gorique, Autonomie morale.
Utilise les distinctions rigoureuses.
Questionne les conditions de possibilit√©."""
    ]
}

# ============================================
# QUESTIONS BAC PAR PHILOSOPHE
# ============================================

QUESTIONS_BAC = {
    "spinoza": [
        "La libert√© est-elle une illusion ?",
        "Suis-je esclave de mes d√©sirs ?",
        "La joie procure-t-elle un pouvoir ?",
        "Peut-on d√©sirer sans souffrir ?",
        "La raison peut-elle tout expliquer ?"
    ],
    "bergson": [
        "Le temps passe-t-il vraiment ?",
        "Se souvenir, est-ce revivre ?",
        "L'art requiert-il de l'inspiration ?",
        "Peut-on se conna√Ætre soi-m√™me ?",
        "La conscience fait-elle notre identit√© ?"
    ],
    "kant": [
        "Agir moralement, est-ce agir par devoir ?",
        "√ätre libre, est-ce faire ce qui nous pla√Æt ?",
        "Que puis-je savoir du monde ?",
        "La morale est-elle universelle ?",
        "Qu'est-ce qu'une soci√©t√© juste ?"
    ]
}

# ============================================
# D√âTECTION CONTEXTUELLE
# ============================================

def detecter_contexte(user_input: str) -> str:
    """D√©tecte le contexte √©motionnel de la r√©ponse"""
    text_lower = user_input.lower()

    # Accord explicite
    if any(re.search(p, text_lower) for p in [r'\boui\b', r'\bd\'accord\b', r'\bexact\b', r'\bc\'est √ßa\b']):
        return "accord"

    # Confusion
    if any(re.search(p, text_lower) for p in [r'comprends? pas', r'je sais pas', r'c\'est quoi']):
        return "confusion"

    # R√©sistance
    if any(re.search(p, text_lower) for p in [r'\bmais\b', r'\bnon\b', r'pas d\'accord', r'faux']):
        return "resistance"

    return "neutre"

# ============================================
# POST-PROCESSING
# ============================================

def nettoyer_reponse(text: str) -> str:
    """Nettoie la r√©ponse g√©n√©r√©e"""
    text = re.sub(r'\([^)]*[Aa]ttends[^)]*\)', '', text)
    text = re.sub(r'[üòÄ-üôèüåÄ-üóøüöÄ-üõø]', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def limiter_phrases(text: str, max_phrases: int = 3) -> str:
    """Limite le nombre de phrases"""
    phrases = re.split(r'[.!?]+\s+', text)
    phrases = [p.strip() for p in phrases if p.strip()]
    if len(phrases) <= max_phrases:
        return text
    return '. '.join(phrases[:max_phrases]) + '.'

# ============================================
# CLASSE DIALOGUE UNIVERSELLE
# ============================================

class DialoguePhilosophe:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def generate_response(self, user_input: str, history: List[Tuple], philosophe: str) -> Dict:
        """G√©n√®re une r√©ponse adapt√©e au philosophe"""
        history = history or []
        contexte = detecter_contexte(user_input)

        # Choisir prompt selon philosophe
        base_prompt = random.choice(PROMPTS_PHILOSOPHES[philosophe])

        system_prompt = base_prompt + """\n\nR√àGLES STRICTES:
- Tutoie toujours l'√©l√®ve (tu/ton/ta)
- Questionne au lieu d'affirmer
- Varie tes formulations
"""

        if contexte == "confusion":
            system_prompt += "\nL'√©l√®ve est confus ‚Üí Donne UNE analogie concr√®te simple."
        elif contexte == "resistance":
            system_prompt += "\nL'√©l√®ve r√©siste ‚Üí R√©v√®le une contradiction dans sa position."
        elif contexte == "accord":
            system_prompt += "\nL'√©l√®ve accepte. AVANCE vers la prochaine √©tape logique."

        # Construction messages
        messages = [{"role": "system", "content": system_prompt}]

        for exchange in history[-4:]:
            if exchange[0]:
                messages.append({"role": "user", "content": exchange[0]})
            if exchange[1]:
                clean_response = exchange[1].split('\n\n*[')[0]
                messages.append({"role": "assistant", "content": clean_response})

        messages.append({"role": "user", "content": user_input})

        # G√©n√©ration
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )

        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )

        response = nettoyer_reponse(response)
        response = limiter_phrases(response, 3)

        return {
            "message": response,
            "contexte": contexte
        }

# ============================================
# CHARGEMENT MOD√àLE
# ============================================

@torch.no_grad()
def load_model():
    """Charge le mod√®le SNB (Qwen 14B + LoRA)"""
    # 4-bit au lieu de 8-bit : ~7GB au lieu de 14GB, tient facilement dans T4 16GB
    quantization_config = BitsAndBytesConfig(
        load_in_4bit=True,  # 4-bit : moiti√© moins de VRAM n√©cessaire
        bnb_4bit_compute_dtype=torch.float16,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4"  # NormalFloat4 - meilleure qualit√© pour 4-bit
    )

    print("üîÑ Chargement Qwen 14B (4-bit) sur GPU...")
    
    # T4 a 16GB VRAM - Qwen 14B 4-bit fait ~7GB, donc largement suffisant
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=quantization_config,
        device_map="auto",  # Auto devrait maintenant tout mettre sur GPU
        token=HF_TOKEN,
        trust_remote_code=True
    )

    print("üîÑ Chargement tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,
        token=HF_TOKEN,
        trust_remote_code=True
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("üîÑ Application LoRA Spinoza Niveau B...")
    model = PeftModel.from_pretrained(
        base_model,
        ADAPTER_MODEL,
        token=HF_TOKEN
    )

    print("‚úÖ Mod√®le charg√© avec succ√®s!")
    return model, tokenizer

# ============================================
# INTERFACE GRADIO 3 PHILOSOPHES
# ============================================

def create_interface():
    """Interface Gradio avec 3 onglets"""

    print("üîÑ Initialisation mod√®le...")
    model, tokenizer = load_model()
    dialogue = DialoguePhilosophe(model, tokenizer)

    def init_conversation(philosophe):
        """Initialise avec une question du bac"""
        question = random.choice(QUESTIONS_BAC[philosophe])
        noms = {"spinoza": "Spinoza", "bergson": "Henri Bergson", "kant": "Emmanuel Kant"}
        return [[None, f"Bonjour ! Je suis {noms[philosophe]}. Discutons ensemble de cette question :\n\n**{question}**\n\nQu'en penses-tu ?"]]

    def chat_function(message, history, philosophe):
        """Fonction chat universelle"""
        if not message.strip():
            return "", history

        try:
            history = history or []
            result = dialogue.generate_response(message, history, philosophe)
            response = result["message"]
            contexte = result["contexte"]

            history.append([message, f"{response}\n\n*[Contexte: {contexte}]*"])
            return "", history

        except Exception as e:
            print(f"[ERROR] {str(e)}")
            history = history or []
            history.append([message, f"Erreur: {str(e)}"])
            return "", history

    # Fonctions sp√©cifiques pour l'API
    def chat_spinoza(message, history):
        return chat_function(message, history, "spinoza")

    def chat_bergson(message, history):
        return chat_function(message, history, "bergson")

    def chat_kant(message, history):
        return chat_function(message, history, "kant")

    # Interface avec Tabs
    with gr.Blocks(title="Bergson & Friends - SNB") as interface:
        gr.Markdown("# üé≠ Bergson & Friends - Spinoza Niveau B")
        gr.Markdown("*Un seul mod√®le (Qwen 14B + LoRA Niveau B) pour 3 philosophes*")

        with gr.Tabs():
            # SPINOZA
            with gr.Tab("üî∑ Spinoza"):
                chatbot_spinoza = gr.Chatbot(
                    value=init_conversation("spinoza"),
                    height=500
                )
                msg_spinoza = gr.Textbox(placeholder="R√©ponds √† Spinoza...", container=False)
                with gr.Row():
                    nouveau_spinoza = gr.Button("üé≤ Nouvelle question")
                    clear_spinoza = gr.Button("üóëÔ∏è Effacer")

                msg_spinoza.submit(
                    chat_spinoza,
                    [msg_spinoza, chatbot_spinoza],
                    [msg_spinoza, chatbot_spinoza],
                    api_name="chat_spinoza"
                )
                nouveau_spinoza.click(lambda: init_conversation("spinoza"), None, chatbot_spinoza)
                clear_spinoza.click(lambda: ([], None), None, [chatbot_spinoza, msg_spinoza])

            # BERGSON
            with gr.Tab("üîµ Bergson"):
                chatbot_bergson = gr.Chatbot(
                    value=init_conversation("bergson"),
                    height=500
                )
                msg_bergson = gr.Textbox(placeholder="R√©ponds √† Bergson...", container=False)
                with gr.Row():
                    nouveau_bergson = gr.Button("üé≤ Nouvelle question")
                    clear_bergson = gr.Button("üóëÔ∏è Effacer")

                msg_bergson.submit(
                    chat_bergson,
                    [msg_bergson, chatbot_bergson],
                    [msg_bergson, chatbot_bergson],
                    api_name="chat_bergson"
                )
                nouveau_bergson.click(lambda: init_conversation("bergson"), None, chatbot_bergson)
                clear_bergson.click(lambda: ([], None), None, [chatbot_bergson, msg_bergson])

            # KANT
            with gr.Tab("üü£ Kant"):
                chatbot_kant = gr.Chatbot(
                    value=init_conversation("kant"),
                    height=500
                )
                msg_kant = gr.Textbox(placeholder="R√©ponds √† Kant...", container=False)
                with gr.Row():
                    nouveau_kant = gr.Button("üé≤ Nouvelle question")
                    clear_kant = gr.Button("üóëÔ∏è Effacer")

                msg_kant.submit(
                    chat_kant,
                    [msg_kant, chatbot_kant],
                    [msg_kant, chatbot_kant],
                    api_name="chat_kant"
                )
                nouveau_kant.click(lambda: init_conversation("kant"), None, chatbot_kant)
                clear_kant.click(lambda: ([], None), None, [chatbot_kant, msg_kant])

        gr.Markdown("---")
        gr.Markdown("**Mod√®le :** Qwen 14B + LoRA Spinoza Niveau B | **Contextes :** accord, confusion, r√©sistance, neutre")

    return interface

# ============================================
# LANCEMENT
# ============================================

if __name__ == "__main__":
    print("üöÄ Lancement SNB avec 3 API endpoints: chat_spinoza, chat_bergson, chat_kant")
    interface = create_interface()
    interface.queue()
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        show_api=True
    )
