import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import re
import random
from typing import Dict, List, Tuple
import os

# ============================================
# CONFIGURATION
# ============================================

BASE_MODEL = "Qwen/Qwen2.5-14B-Instruct"
ADAPTER_MODEL = "FJDaz/qwen-spinoza-niveau-b"
HF_TOKEN = os.getenv("HF_TOKEN")

# ============================================
# D√âTECTION CONTEXTUELLE V2 (CODE SUCC√àS)
# ============================================

def detecter_oui_explicite(user_input: str) -> bool:
    patterns = [
        r'\boui\b', r'\byep\b', r'\byes\b', r'\bexact\b',
        r'\bd\'accord\b', r'\bok\b', r'\btout √† fait\b',
        r'\bc\'est √ßa\b', r'\bvoil√†\b'
    ]
    text_lower = user_input.lower()
    return any(re.search(pattern, text_lower) for pattern in patterns)

def detecter_confusion(user_input: str) -> bool:
    patterns = [
        r'comprends? pas', r'vois pas', r'c\'est quoi',
        r'je sais pas', r'j\'en sais rien', r'pourquoi',
        r'rapport', r'quel lien', r'chelou', r'dingue'
    ]
    text_lower = user_input.lower()
    return any(re.search(pattern, text_lower) for pattern in patterns)

def detecter_resistance(user_input: str) -> bool:
    patterns = [
        r'\bmais\b', r'\bnon\b', r'pas d\'accord', r'faux',
        r'n\'importe quoi', r'pas vrai', r'je peux',
        r'bullshit', r'chiant'
    ]
    text_lower = user_input.lower()
    return any(re.search(pattern, text_lower) for pattern in patterns)

def detecter_contexte(user_input: str) -> str:
    """D√©tecte contexte selon logique V2 succ√®s"""
    if detecter_oui_explicite(user_input):
        return "accord"
    elif detecter_confusion(user_input):
        return "confusion"
    elif detecter_resistance(user_input):
        return "resistance"
    else:
        return "neutre"

# ============================================
# SYSTEM PROMPTS ADAPTATIFS V2
# ============================================

SYSTEM_PROMPTS_BASE = [
    """Tu es Spinoza incarn√©. Tu dialogues avec un √©l√®ve pour le guider vers la compr√©hension.
Utilise les sch√®mes logiques pour structurer ton raisonnement.
Varie tes transitions: "Donc", "MAIS ALORS", "Imagine", "Cela implique", etc.
Sois p√©dagogique mais rigoureux. Pose des questions pour faire r√©fl√©chir.""",

    """Tu es un tuteur philosophique spinoziste. Guide l'√©l√®ve vers la clart√© par le dialogue.
Applique les sch√®mes logiques selon le contexte.
Utilise "MAIS ALORS" pour r√©v√©ler les contradictions. Varie tes formulations.
Fais progresser l'√©l√®ve √©tape par √©tape.""",

    """Tu enseignes Spinoza par le questionnement socratique.
D√©tecte les confusions de l'√©l√®ve et applique le sch√®me logique adapt√©.
Transitions vari√©es: "Donc", "Imagine", "C'est contradictoire", "Cela implique".
Reste concis mais pr√©cis."""
]

def construire_prompt_contextuel_v2(contexte: str) -> str:
    """Code exact V2 succ√®s"""
    base = random.choice(SYSTEM_PROMPTS_BASE)
    
    base += """\n\nR√àGLES STRICTES:
- Tutoie toujours l'√©l√®ve (tu/ton/ta)
- Questionne au lieu d'affirmer
- Varie tes formulations
"""

    if contexte == "confusion":
        base += "\nL'√©l√®ve est confus ‚Üí Donne UNE analogie concr√®te simple."
    elif contexte == "resistance":
        base += "\nL'√©l√®ve r√©siste ‚Üí R√©v√®le une contradiction dans sa position."
    elif contexte == "accord":
        base += "\nL'√©l√®ve accepte. AVANCE vers la prochaine √©tape logique."
    else:
        base += "\n√âl√®ve neutre ‚Üí Pose une question pour faire r√©fl√©chir."

    return base

# ============================================
# POST-PROCESSING V2
# ============================================

def nettoyer_reponse(text: str) -> str:
    """Code exact succ√®s V2"""
    # Annotations m√©ta
    text = re.sub(r'\([^)]*[Aa]ttends[^)]*\)', '', text)
    text = re.sub(r'\([^)]*[Pp]oursuis[^)]*\)', '', text)
    text = re.sub(r'\([^)]*[Dd]onne[^)]*\)', '', text)
    
    # Emojis (le mod√®le en g√©n√®re parfois)
    text = re.sub(r'[üòÄ-üôèüåÄ-üóøüöÄ-üõø]', '', text)
    
    # Nettoyer espaces
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'\s+([.!?])', r'\1', text)
    
    return text

def limiter_phrases(text: str, max_phrases: int = 3) -> str:
    """Limite nombre de phrases"""
    phrases = re.split(r'[.!?]+\s+', text)
    phrases = [p.strip() for p in phrases if p.strip()]
    
    if len(phrases) <= max_phrases:
        return text
    
    return '. '.join(phrases[:max_phrases]) + '.'

# ============================================
# CLASSE DIALOGUE V2 (CODE SUCC√àS)
# ============================================

class DialogueSpinozaV2:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def generate_response(self, user_input: str, history: List[Tuple] = None) -> Dict:
        """G√©n√®re r√©ponse avec adaptation contextuelle V2

        Args:
            user_input: Message utilisateur actuel
            history: Historique format Gradio [[user_msg, bot_msg], ...]
        """
        # Utiliser l'historique fourni
        history = history or []

        # D√©tection contexte
        contexte = detecter_contexte(user_input)

        # Construction prompt adaptatif
        system_prompt = construire_prompt_contextuel_v2(contexte)

        # Logique climax pour r√©sistance (code REF)
        climax_utilise = False
        if contexte == "resistance":
            if len(history) > 3:  # Au moins 3 √©changes
                system_prompt += "\nR√âV√àLE LA CONTRADICTION MAJEURE. Sois percutant."
                climax_utilise = True

        # Historique conversation
        messages = [{"role": "system", "content": system_prompt}]

        # Ajouter historique (4 derniers √©changes max)
        for exchange in history[-4:]:
            if exchange[0]:  # user message
                messages.append({"role": "user", "content": exchange[0]})
            if exchange[1]:  # assistant message
                # Nettoyer annotation contexte de la r√©ponse pr√©c√©dente
                clean_response = exchange[1].split('\n\n*[Contexte:')[0]
                messages.append({"role": "assistant", "content": clean_response})

        # Message actuel
        messages.append({"role": "user", "content": user_input})
        
        # Formatage
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        # G√©n√©ration avec param√®tres V2
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                top_p=0.9,
                do_sample=True,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        )
        
        # Post-processing V2
        response = nettoyer_reponse(response)
        response = limiter_phrases(response, 3)

        return {
            "message": response,
            "contexte": contexte
        }

# ============================================
# CHARGEMENT MOD√àLE (8-BIT CRITIQUE)
# ============================================

@torch.no_grad()
def load_model():
    """Chargement avec quantization 8-bit (FIX CRITIQUE)"""
    
    # Configuration 8-bit (FIX EOS)
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=True,  # CRITIQUE: 8-bit pas 4-bit
        llm_int8_threshold=6.0,
        llm_int8_has_fp16_weight=False,
    )
    
    print("üîÑ Chargement Qwen 14B (8-bit)...")
    
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=quantization_config,
        device_map="auto",
        torch_dtype=torch.float16,
        token=HF_TOKEN,
        trust_remote_code=True
    )
    
    print("üîÑ Chargement tokenizer...")

    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL,  # CRITIQUE: Charger depuis base model, pas adapter
        token=HF_TOKEN,
        trust_remote_code=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("üîÑ Application LoRA Spinoza Niveau B...")
    
    model = PeftModel.from_pretrained(
        base_model,
        ADAPTER_MODEL,
        token=HF_TOKEN
    )
    
    print("‚úÖ Mod√®le charg√© avec succ√®s!")
    
    return model, tokenizer

# ============================================
# QUESTIONS ANNALES BAC (AMORCES)
# ============================================

QUESTIONS_BAC_SPINOZA = [
    "La libert√© est-elle une illusion ?",
    "Suis-je esclave de mes d√©sirs ?", 
    "Puis-je ma√Ætriser mes √©motions ?",
    "La joie procure-t-elle un pouvoir ?",
    "Peut-on d√©sirer sans souffrir ?",
    "Sommes-nous responsables de nos actes ?",
    "La raison suffit-elle √† nous guider ?",
    "Peut-on vivre sans passions ?",
    "Qu'est-ce que la v√©rit√© ?",
    "Que puis-je savoir du monde ?",
    "Dieu et Nature sont-ils identiques ?",
    "Pourquoi souffrons-nous ?",
    "La connaissance rend-elle libre ?",
    "Peut-on √™tre heureux sans √™tre libre ?",
    "L'homme est-il naturellement social ?"
]

def choisir_question_amorce() -> str:
    """Choisit une question al√©atoire du bac"""
    return random.choice(QUESTIONS_BAC_SPINOZA)

# ============================================
# INTERFACE GRADIO
# ============================================

def create_interface():
    """Interface Gradio optimis√©e"""
    
    print("üîÑ Initialisation mod√®le...")
    model, tokenizer = load_model()
    dialogue = DialogueSpinozaV2(model, tokenizer)
    
    def initialiser_conversation():
        """Spinoza d√©marre avec une question"""
        question = choisir_question_amorce()
        return [[None, f"Bonjour ! Je suis Spinoza. Discutons ensemble de cette question :\n\n**{question}**\n\nQu'en penses-tu ?"]]
    
    def chat_function(message, history):
        """Fonction chat Gradio - SYST√àME ADAPTATIF avec historique"""
        if not message.strip():
            return "", history

        try:
            # Utiliser l'historique fourni par le frontend (SYST√àME ADAPTATIF)
            history = history or []

            result = dialogue.generate_response(message, history)
            response = result["message"]
            contexte = result["contexte"]

            # Format historique Gradio
            history.append([message, f"{response}\n\n*[Contexte: {contexte}]*"])

            return "", history

        except Exception as e:
            error_msg = f"Erreur: {str(e)}"
            print(f"[ERROR] {str(e)}")
            history = history or []
            history.append([message, error_msg])
            return "", history
    
    # Interface Gradio
    with gr.Blocks(title="Spinoza Niveau B - V2") as interface:
        gr.Markdown("# üé≠ Spinoza - Dialogue Philosophique V2")
        gr.Markdown("*Mod√®le fine-tun√© niveau B avec d√©tection contextuelle - Spinoza pose une question du bac pour d√©marrer*")
        
        chatbot = gr.Chatbot(
            value=initialiser_conversation(),  # Spinoza d√©marre
            elem_id="chatbot",
            bubble_full_width=False,
            height=500
        )
        
        msg = gr.Textbox(
            placeholder="R√©ponds √† Spinoza ou pose une nouvelle question...",
            container=False,
            scale=7
        )
        
        nouveau_btn = gr.Button("üé≤ Nouvelle question", scale=1)
        clear = gr.Button("üóëÔ∏è Effacer", scale=1)
        
        msg.submit(chat_function, [msg, chatbot], [msg, chatbot])
        nouveau_btn.click(lambda: initialiser_conversation(), None, chatbot)
        clear.click(lambda: ([], None), None, [chatbot, msg])
        
        gr.Markdown("---")
        gr.Markdown("**Contextes d√©tect√©s:** accord, confusion, r√©sistance, neutre")
        gr.Markdown("**Questions bac:** 15 sujets philosophiques authentiques")
    
    return interface

# ============================================
# LANCEMENT
# ============================================

if __name__ == "__main__":
    interface = create_interface()
    interface.queue()  # Important pour l'API
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,
        show_api=True  # Active l'API
    )