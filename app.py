import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import re
import random
from typing import Dict, List, Tuple
import os

# ============================================
# CONFIGURATION
# ============================================

BASE_MODEL = "Qwen/Qwen2.5-14B-Instruct"
ADAPTER_MODEL = "FJDaz/qwen-spinoza-niveau-b"
HF_TOKEN = os.getenv("HF_TOKEN")

# ============================================
# D√âTECTION CONTEXTUELLE V2 (CODE SUCC√àS)
# ============================================

def detecter_oui_explicite(user_input: str) -> bool:
    patterns = [
        r'\boui\b', r'\byep\b', r'\byes\b', r'\bexact\b',
        r'\bd\'accord\b', r'\bok\b', r'\btout √† fait\b',
        r'\bc\'est √ßa\b', r'\bvoil√†\b'
    ]
    text_lower = user_input.lower()
    return any(re.search(pattern, text_lower) for pattern in patterns)

def detecter_confusion(user_input: str) -> bool:
    patterns = [
        r'comprends? pas', r'vois pas', r'c\'est quoi',
        r'je sais pas', r'j\'en sais rien', r'pourquoi',
        r'rapport', r'quel lien', r'chelou', r'dingue'
    ]
    text_lower = user_input.lower()
    return any(re.search(pattern, text_lower) for pattern in patterns)

def detecter_resistance(user_input: str) -> bool:
    patterns = [
        r'\bmais\b', r'\bnon\b', r'pas d\'accord', r'faux',
        r'n\'importe quoi', r'pas vrai', r'je peux',
        r'bullshit', r'chiant'
    ]
    text_lower = user_input.lower()
    return any(re.search(pattern, text_lower) for pattern in patterns)

def detecter_contexte(user_input: str) -> str:
    """D√©tecte contexte selon logique V2 succ√®s"""
    if detecter_oui_explicite(user_input):
        return "accord"
    elif detecter_confusion(user_input):
        return "confusion"
    elif detecter_resistance(user_input):
        return "resistance"
    else:
        return "neutre"

# ============================================
# SYSTEM PROMPTS ADAPTATIFS V2
# ============================================

SYSTEM_PROMPTS_BASE = [
    """Tu es Spinoza incarn√©. Tu dialogues avec un √©l√®ve pour le guider vers la compr√©hension.
Utilise les sch√®mes logiques pour structurer ton raisonnement.
Varie tes transitions: "Donc", "MAIS ALORS", "Imagine", "Cela implique", etc.
Sois p√©dagogique mais rigoureux. Pose des questions pour faire r√©fl√©chir.""",

    """Tu es un tuteur philosophique spinoziste. Guide l'√©l√®ve vers la clart√© par le dialogue.
Applique les sch√®mes logiques selon le contexte.
Utilise "MAIS ALORS" pour r√©v√©ler les contradictions. Varie tes formulations.
Fais progresser l'√©l√®ve √©tape par √©tape.""",

    """Tu enseignes Spinoza par le questionnement socratique.
D√©tecte les confusions de l'√©l√®ve et applique le sch√®me logique adapt√©.
Transitions vari√©es: "Donc", "Imagine", "C'est contradictoire", "Cela implique".
Reste concis mais pr√©cis."""
]

def construire_prompt_contextuel_v2(contexte: str) -> str:
    """Code exact V2 succ√®s"""
    base = random.choice(SYSTEM_PROMPTS_BASE)

    if contexte == "confusion":
        base += "\n\nL'√©l√®ve est confus. Utilise une ANALOGIE concr√®te pour clarifier."
    elif contexte == "resistance":
        base += "\n\nL'√©l√®ve r√©siste. R√©v√®le la CONTRADICTION de sa position."
    elif contexte == "accord":
        base += "\n\nL'√©l√®ve accepte. AVANCE vers la prochaine √©tape logique."
    else:
        base += "\n\nPose une question pour faire r√©fl√©chir l'√©l√®ve."

    return base

# ============================================
# POST-PROCESSING V2
# ============================================

def nettoyer_reponse(text: str) -> str:
    """Code exact succ√®s V2"""
    # Annotations m√©ta (probl√®me identifi√© dans tests)
    text = re.sub(r'\([^)]*[Aa]ttends[^)]*\)', '', text)
    text = re.sub(r'\([^)]*[Pp]oursuis[^)]*\)', '', text)
    text = re.sub(r'\([^)]*[Dd]onne[^)]*\)', '', text)
    
    # Emojis inappropri√©s
    text = re.sub(r'[üòÄ-üôèüåÄ-üóøüöÄ-üõø]', '', text)
    
    # Nettoyer espaces
    text = re.sub(r'\s+', ' ', text).strip()
    text = re.sub(r'\s+([.!?])', r'\1', text)
    
    return text

def limiter_phrases(text: str, max_phrases: int = 3) -> str:
    """Limite verbosit√© (probl√®me identifi√©)"""
    phrases = re.split(r'[.!?]+\s+', text)
    phrases = [p.strip() for p in phrases if p.strip()]
    
    if len(phrases) <= max_phrases:
        return text
    
    return '. '.join(phrases[:max_phrases]) + '.'

# ============================================
# CLASSE DIALOGUE V2 (CODE SUCC√àS)
# ============================================

class DialogueSpinozaV2:
    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer
        self.conversation_history = []
        
    def generate_response(self, user_input: str) -> Dict:
        """G√©n√®re r√©ponse avec adaptation contextuelle V2"""
        
        # D√©tection contexte
        contexte = detecter_contexte(user_input)
        
        # Construction prompt adaptatif
        system_prompt = construire_prompt_contextuel_v2(contexte)
        
        # Historique conversation (limit√© pour √©viter surcharge)
        messages = [{"role": "system", "content": system_prompt}]
        
        # Ajouter derniers √©changes
        for entry in self.conversation_history[-4:]:  # 4 derniers √©changes max
            messages.append({"role": "user", "content": entry["user"]})
            messages.append({"role": "assistant", "content": entry["assistant"]})
            
        # Message actuel
        messages.append({"role": "user", "content": user_input})
        
        # Formatage
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer(text, return_tensors="pt").to(self.model.device)
        
        # G√©n√©ration avec param√®tres V2 optimis√©s
        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=150,  # Concis pour √©viter verbosit√©
                temperature=0.7,     # Vari√©t√© sans chaos
                top_p=0.9,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
        
        response = self.tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:], 
            skip_special_tokens=True
        )
        
        # Post-processing V2
        response = nettoyer_reponse(response)
        response = limiter_phrases(response, 3)
        
        # Sauvegarde historique
        self.conversation_history.append({
            "user": user_input,
            "assistant": response,
            "contexte": contexte
        })
        
        return {
            "message": response,
            "contexte": contexte
        }

# ============================================
# QUESTIONS BAC (AMORCES SPINOZA)
# ============================================

QUESTIONS_BAC_SPINOZA = [
    "La libert√© est-elle une illusion ?",
    "Suis-je esclave de mes d√©sirs ?", 
    "Puis-je ma√Ætriser mes √©motions ?",
    "La joie procure-t-elle un pouvoir ?",
    "Peut-on d√©sirer sans souffrir ?",
    "Sommes-nous responsables de nos actes ?",
    "La raison suffit-elle √† nous guider ?",
    "Peut-on vivre sans passions ?",
    "Qu'est-ce que la v√©rit√© ?",
    "Que puis-je savoir du monde ?",
    "Dieu et Nature sont-ils identiques ?",
    "Pourquoi souffrons-nous ?",
    "La connaissance rend-elle libre ?",
    "Peut-on √™tre heureux sans √™tre libre ?",
    "L'homme est-il naturellement social ?"
]

def choisir_question_amorce() -> str:
    """Choisit une question al√©atoire du bac"""
    return random.choice(QUESTIONS_BAC_SPINOZA)

# ============================================
# CHARGEMENT MOD√àLE (8-BIT CRITIQUE)
# ============================================

@torch.no_grad()
def load_model():
    """Chargement avec quantization 8-bit (FIX EOS CRITIQUE)"""
    
    # Configuration 8-bit (CORRECTION CRITIQUE IDENTIFI√âE)
    quantization_config = BitsAndBytesConfig(
        load_in_8bit=True,  # CRITIQUE: 8-bit au lieu de 4-bit
        llm_int8_threshold=6.0,
        llm_int8_has_fp16_weight=False,
    )
    
    print("üîÑ Chargement Qwen 14B base (8-bit)...")
    
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        quantization_config=quantization_config,
        device_map="auto",
        torch_dtype=torch.float16,
        token=HF_TOKEN,
        trust_remote_code=True
    )
    
    print("üîÑ Chargement tokenizer...")
    
    tokenizer = AutoTokenizer.from_pretrained(
        ADAPTER_MODEL,
        token=HF_TOKEN,
        trust_remote_code=True
    )
    
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print("üîÑ Application adaptateurs LoRA Spinoza Niveau B...")
    
    model = PeftModel.from_pretrained(
        base_model,
        ADAPTER_MODEL,
        token=HF_TOKEN
    )
    
    print("‚úÖ Spinoza Niveau B V1 charg√© avec succ√®s!")
    
    return model, tokenizer

# ============================================
# INTERFACE GRADIO SIMPLE
# ============================================

def create_interface():
    """Interface Gradio pour HF Space"""
    
    print("üîÑ Initialisation Spinoza Niveau B V1...")
    model, tokenizer = load_model()
    dialogue = DialogueSpinozaV2(model, tokenizer)
    
    def initialiser_conversation():
        """Spinoza d√©marre avec question bac"""
        question = choisir_question_amorce()
        return [[None, f"üé≠ **Spinoza** : Bonjour ! Explorons ensemble cette question :\n\n**{question}**\n\nQu'en penses-tu ?"]]
    
    def chat_function(message, history):
        """Fonction chat principale"""
        if not message.strip():
            return "", history
            
        try:
            result = dialogue.generate_response(message)
            response = result["message"]
            contexte = result["contexte"]
            
            # Format avec contexte pour debug
            response_formatted = f"üé≠ **Spinoza** : {response}\n\n*[Contexte d√©tect√©: {contexte}]*"
            
            history = history or []
            history.append([message, response_formatted])
            
            return "", history
            
        except Exception as e:
            error_msg = f"‚ùå Erreur: {str(e)}"
            history = history or []
            history.append([message, error_msg])
            return "", history
    
    # Interface Gradio
    with gr.Blocks(title="Spinoza Niveau B V1") as interface:
        gr.Markdown("# üé≠ Spinoza - Dialogue Philosophique Niveau B")
        gr.Markdown("*Mod√®le fine-tun√© avec d√©tection contextuelle - Code Colab V1 fonctionnel*")
        
        chatbot = gr.Chatbot(
            value=initialiser_conversation(),
            elem_id="chatbot",
            bubble_full_width=False,
            height=600,
            show_label=False
        )
        
        with gr.Row():
            msg = gr.Textbox(
                placeholder="R√©ponds √† Spinoza ou pose une question...",
                container=False,
                scale=8
            )
            nouveau_btn = gr.Button("üé≤ Nouvelle question", scale=2)
            clear_btn = gr.Button("üóëÔ∏è Effacer", scale=1)
        
        msg.submit(chat_function, [msg, chatbot], [msg, chatbot])
        nouveau_btn.click(lambda: initialiser_conversation(), None, chatbot)
        clear_btn.click(lambda: ([], ""), None, [chatbot, msg])
        
        gr.Markdown("---")
        gr.Markdown("**üéØ Contextes d√©tect√©s :** accord, confusion, r√©sistance, neutre")
        gr.Markdown("**üìö Questions bac :** 15 sujets philosophiques authentiques Spinoza")
        gr.Markdown("**‚öôÔ∏è Mod√®le :** Qwen 14B + LoRA Spinoza Niveau B (quantization 8-bit)")
    
    return interface

# ============================================
# LANCEMENT
# ============================================

if __name__ == "__main__":
    interface = create_interface()
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False
    )